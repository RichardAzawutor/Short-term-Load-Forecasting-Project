{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-summary"
      ],
      "metadata": {
        "id": "RV7EilWaAhYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVj-uGvZCE-0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import gspread\n",
        "\n",
        "# Mounting Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_test = pd.read_excel(\"/content/drive/MyDrive/Elorm's Work/test_dataframes.xlsx\")\n",
        "df_train = pd.read_excel(\"/content/drive/MyDrive/Elorm's Work/train_dataframes.xlsx\")"
      ],
      "metadata": {
        "id": "P4q0o_0ICb5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "s0H3-pnzEZhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING A NORMAL NEURAL NETWORK FOR SHORT-TERM LOAD FORECASTING\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from datetime import datetime\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "df_train.dropna(inplace=True)\n",
        "df_test.dropna(inplace=True)\n",
        "\n",
        "df_train['datetime'] = pd.to_datetime(df_train['datetime'])\n",
        "df_test['datetime'] = pd.to_datetime(df_test['datetime'])\n",
        "\n",
        "df_train['year'] = df_train['datetime'].dt.year\n",
        "df_train['month'] = df_train['datetime'].dt.month\n",
        "df_train['day'] = df_train['datetime'].dt.day\n",
        "df_train['hour'] = df_train['datetime'].dt.hour\n",
        "df_test['year'] = df_test['datetime'].dt.year\n",
        "df_test['month'] = df_test['datetime'].dt.month\n",
        "df_test['day'] = df_test['datetime'].dt.day\n",
        "df_test['hour'] = df_test['datetime'].dt.hour\n",
        "\n",
        "df_train.drop(columns=['datetime'], inplace=True)\n",
        "df_test.drop(columns=['datetime'], inplace=True)\n",
        "\n",
        "X_train = df_train.drop(columns=['DEMAND']).values\n",
        "y_train = df_train['DEMAND'].values\n",
        "X_test = df_test.drop(columns=['DEMAND']).values\n",
        "y_test = df_test['DEMAND'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "class EnergyDemandDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "# create datasets and dataloaders\n",
        "train_dataset = EnergyDemandDataset(X_train, y_train)\n",
        "test_dataset = EnergyDemandDataset(X_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "class ANNModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ANNModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "model = ANNModel(input_dim)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 350\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\") # if you want to suppress the results, comment this out\n",
        "\n",
        "def evaluate_model(loader, model):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            outputs = model(inputs)\n",
        "            predictions.extend(outputs.squeeze().tolist())\n",
        "            actuals.extend(targets.tolist())\n",
        "    return np.array(predictions), np.array(actuals)\n",
        "\n",
        "# evaluate the model with the actual test data\n",
        "predictions, actuals = evaluate_model(test_loader, model)\n",
        "\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n",
        "\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"MAPE: {mape}%\")"
      ],
      "metadata": {
        "id": "3Zc6mQd4CJGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING A CONVOLUTIONAL NEURAL NETWORK FOR SHORT-TERM LOAD FORECASTING\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "df_train.dropna(inplace=True)\n",
        "df_test.dropna(inplace=True)\n",
        "\n",
        "X_train = df_train.drop(columns=['DEMAND']).values\n",
        "y_train = df_train['DEMAND'].values\n",
        "X_test = df_test.drop(columns=['DEMAND']).values\n",
        "y_test = df_test['DEMAND'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "class EnergyDemandDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "train_dataset = EnergyDemandDataset(X_train, y_train)\n",
        "test_dataset = EnergyDemandDataset(X_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "class ComplexCNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ComplexCNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self._to_linear = None\n",
        "        self.convs = nn.Sequential(\n",
        "            self.conv1,\n",
        "            nn.ReLU(),\n",
        "            self.pool,\n",
        "            self.conv2,\n",
        "            nn.ReLU(),\n",
        "            self.pool,\n",
        "            self.conv3,\n",
        "            nn.ReLU(),\n",
        "            self.pool\n",
        "        )\n",
        "        x = torch.randn(64, 1, X_train.shape[2]).float()  # batch size, channels, features\n",
        "        self._to_linear = self.convs(x).view(x.size(0), -1).shape[1]\n",
        "\n",
        "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "model = ComplexCNNModel()\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 350\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(loader, model):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            outputs = model(inputs)\n",
        "            predictions.extend(outputs.squeeze().tolist())\n",
        "            actuals.extend(targets.tolist())\n",
        "    return np.array(predictions), np.array(actuals)\n",
        "\n",
        "predictions, actuals = evaluate_model(test_loader, model)\n",
        "\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n",
        "\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"MAPE: {mape}%\")\n"
      ],
      "metadata": {
        "id": "AMOBBL37H_xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING A RECURRENT NEURAL NETWORK FOR SHORT-TERM LOAD FORECASTING\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "df_train.dropna(inplace=True)\n",
        "df_test.dropna(inplace=True)\n",
        "\n",
        "X_train = df_train.drop(columns=['DEMAND']).values\n",
        "y_train = df_train['DEMAND'].values\n",
        "X_test = df_test.drop(columns=['DEMAND']).values\n",
        "y_test = df_test['DEMAND'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "seq_length = 1\n",
        "X_train = X_train.reshape(X_train.shape[0], seq_length, -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], seq_length, -1)\n",
        "\n",
        "class EnergyDemandDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "train_dataset = EnergyDemandDataset(X_train, y_train)\n",
        "test_dataset = EnergyDemandDataset(X_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc1(out[:, -1, :])\n",
        "        out = torch.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = torch.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "input_dim = X_train.shape[2]\n",
        "hidden_dim = 128\n",
        "num_layers = 3\n",
        "\n",
        "model = RNNModel(input_dim, hidden_dim, num_layers)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 350\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(loader, model):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            outputs = model(inputs)\n",
        "            predictions.extend(outputs.squeeze().tolist())\n",
        "            actuals.extend(targets.tolist())\n",
        "    return np.array(predictions), np.array(actuals)\n",
        "\n",
        "predictions, actuals = evaluate_model(test_loader, model)\n",
        "\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n",
        "\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"MAPE: {mape}%\")\n"
      ],
      "metadata": {
        "id": "hNjMYOi8IC5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING A LONG SHORT-TERM NEURAL NETWORK MODEL FOR SHORT-TERM LOAD FORECASTING\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "X_train = df_train.drop(columns=['DEMAND']).values\n",
        "y_train = df_train['DEMAND'].values\n",
        "X_test = df_test.drop(columns=['DEMAND']).values\n",
        "y_test = df_test['DEMAND'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "seq_length = 1\n",
        "X_train = X_train.reshape(X_train.shape[0], seq_length, -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], seq_length, -1)\n",
        "\n",
        "class EnergyDemandDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "train_dataset = EnergyDemandDataset(X_train, y_train)\n",
        "test_dataset = EnergyDemandDataset(X_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "class ComplexLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_prob):\n",
        "        super(ComplexLSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_prob)\n",
        "        self.fc1 = nn.Linear(hidden_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])  # added a dropout for shege reasons. but keep it here. it breaks without it for some reason.\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.fc2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.fc3(out))\n",
        "        out = self.fc4(out)\n",
        "        return out\n",
        "\n",
        "input_dim = X_train.shape[2]\n",
        "hidden_dim = 256\n",
        "num_layers = 3\n",
        "dropout_prob = 0.3\n",
        "\n",
        "model = ComplexLSTMModel(input_dim, hidden_dim, num_layers, dropout_prob)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 350\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(loader, model):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            outputs = model(inputs)\n",
        "            predictions.extend(outputs.squeeze().tolist())\n",
        "            actuals.extend(targets.tolist())\n",
        "    return np.array(predictions), np.array(actuals)\n",
        "\n",
        "predictions, actuals = evaluate_model(test_loader, model)\n",
        "\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n",
        "\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"MAPE: {mape}%\")"
      ],
      "metadata": {
        "id": "wKt6Xf-YIEmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING A CNN-LSTM MODEL FOR SHORT-TERM LOAD FORECASTING\n",
        "\n",
        "from torchsummary import summary\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "X_train = df_train.drop(columns=['DEMAND']).values\n",
        "y_train = df_train['DEMAND'].values\n",
        "X_test = df_test.drop(columns=['DEMAND']).values\n",
        "y_test = df_test['DEMAND'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "seq_length = 1\n",
        "X_train = X_train.reshape(X_train.shape[0], seq_length, -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], seq_length, -1)\n",
        "\n",
        "class EnergyDemandDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "train_dataset = EnergyDemandDataset(X_train, y_train)\n",
        "test_dataset = EnergyDemandDataset(X_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "class HybridCNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, cnn_out_channels, lstm_hidden_dim, lstm_num_layers, fc_hidden_dim, dropout_prob):\n",
        "        super(HybridCNNLSTMModel, self).__init__()\n",
        "        self.cnn_out_channels = cnn_out_channels\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.lstm_num_layers = lstm_num_layers\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=cnn_out_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=cnn_out_channels, out_channels=cnn_out_channels, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        cnn_output_dim = (input_dim + 1) // 2 * cnn_out_channels\n",
        "\n",
        "        self.lstm = nn.LSTM(cnn_output_dim, lstm_hidden_dim, lstm_num_layers, batch_first=True, dropout=dropout_prob)\n",
        "\n",
        "        self.fc1 = nn.Linear(lstm_hidden_dim, fc_hidden_dim)\n",
        "        self.fc2 = nn.Linear(fc_hidden_dim, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, input_dim = x.shape\n",
        "        x = x.view(batch_size * seq_len, 1, input_dim)\n",
        "\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "\n",
        "        cnn_out_dim = x.shape[2]\n",
        "        x = x.view(batch_size, seq_len, cnn_out_dim * self.cnn_out_channels)\n",
        "\n",
        "        h0 = torch.zeros(self.lstm_num_layers, batch_size, self.lstm_hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm_num_layers, batch_size, self.lstm_hidden_dim).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "input_dim = X_train.shape[2]\n",
        "cnn_out_channels = 64\n",
        "lstm_hidden_dim = 128\n",
        "lstm_num_layers = 2\n",
        "fc_hidden_dim = 256\n",
        "dropout_prob = 0.3\n",
        "\n",
        "model = HybridCNNLSTMModel(input_dim, cnn_out_channels, lstm_hidden_dim, lstm_num_layers, fc_hidden_dim, dropout_prob)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "summary(model)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(loader, model):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            outputs = model(inputs)\n",
        "            predictions.extend(outputs.squeeze().tolist())\n",
        "            actuals.extend(targets.tolist())\n",
        "    return np.array(predictions), np.array(actuals)\n",
        "\n",
        "predictions, actuals = evaluate_model(test_loader, model)\n",
        "\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n",
        "\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"MAPE: {mape}%\")"
      ],
      "metadata": {
        "id": "03o7NBn8IHpg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
